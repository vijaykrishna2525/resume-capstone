<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>My Resume</title>
</head>
<body>
 <h1>Resume</h1>
 <hr />
 <img src="./IMG_7633.jpg" alt="Profile Picture" height="400"/>
 <h2>Summary</h2>   
 <ul>
    <li>Over 2+ years of experience in Data Analytics and Supply Chain solutions with a strong academic foundation in
Data Science and Machine Learning.</li>
    <li>Actively involved in all phases of the Data Analytics Lifecycle, including data collection, cleaning, modeling,
visualization, and interpretation.</li>
    <li>Proficient in programming languages including Python, SQL, R, and Bash, with a solid grasp of data structures,
algorithmic logic, and statistical modeling.</li>
    <li>Experience working in Supply Chain domains across Retail and FMCG clients like Burger King - ZAMP (Brazil), and
Albert Heijn (Netherlands) at o9 Solutions, Inc.</li>
    <li>Hands-on expertise in Forecasting Techniques (ARIMA, TES, TBATS, Random Forest) and Time Series Segmentation
based on lifecycle state, intermittency, and volume.</li>
    <li>Developed and automated end-to-end data pipelines using Python, SQL, SSIS, and o9's proprietary scripting
language (IBPL) for segmentation, forecasting, and batch orchestration.</li>
    <li>Strong understanding of Data Engineering tools and frameworks such as Databricks, Delta Lake, Apache Spark,
Apache Parquet, and Snowflake.</li>
    <li>Proficient in building and deploying data workflows using Databricks Workflows, SSIS, and Azure Data Factory.</li>
    <li>Experience implementing AWS Cloud Services (EC2, S3, Lambda, Glue, CloudWatch) and Microsoft Azure(Synapse,
Data Factory, Azure Functions).</li>
    <li>Knowledge of Security and Authentication systems in web applications; hands-on with Postman, Git, DBFS, and
API integrations.</li>
    <li>Academic project experience in designing and developing machine learning models using scikit-learn, XGBoost,
Random Forest, PySpark, NumPy, Pandas, and Hugging Face for tasks including:
        <ul>
            <li>Mistake Identification in AI Tutors</li>
            <li>Mental Health Analysis based on Work Location</li>
            <li>Bridge Condition Prediction using ML</li>
        </ul>
    </li>
    <li>Experience in database systems like MySQL, PostgreSQL, Snowflake, with advanced proficiency in Stored
Procedures, Data Validation, and Batch Scheduling.</li>
    <li>Developed BI Dashboards and KPIs using Tableau, Power BI, and Microsoft Excel to deliver actionable insights.</li>
    <li>Certified in AWS Cloud Foundations, AWS Associate Solutions Architect, Databricks Lakehouse Fundamentals,
Snowflake Essentials, Tableau, and MySQL.</li>
    <li>Strong grasp of Agile Methodology (SCRUM) with experience documenting sprints, analyzing feasibility, and
delivering to client specifications.</li>
    <li>Recognized for strong interpersonal and communication skills, with a proactive approach to problem-solving and a
passion for learning emerging tools and technologies in the data ecosystem.</li>
 </ul>
 <h2>Education</h2>
 <ul>
    <li>George Mason University, Fairfax, VA (2024 – 2025)
        <ul>
            <li>MS, Data Analytics Engineering – GPA: 4.00</li>
        </ul>
    </li>
    <li>R.V. College of Engineering, Bangalore (2018 – 2022)
        <ul>
            <li>Bachelor of Technology, Civil Engineering – CGPA: 7.51/10</li>
        </ul>
    </li>
 </ul>
 <h2>Work Experience</h2>
 <h3>Role: Functional Consultant, (Sep 2022 – Dec 2023)</h3>
 <h4>Description:</h4>
 <p>o9 Solutions is a leading AI-powered integrated planning and operations platform trusted by global
enterprises. The company helps businesses in retail, manufacturing, and FMCG to digitally transform their planning and
decision-making processes. During my tenure, I supported multiple clients across supply chain transformation initiatives
by leveraging statistical forecasting, segmentation techniques, and demand/supply planning solutions.</p>
<h4>Responsibilities:</h4>
<ul>
    <li>Collaborated with clients like Burger King, ZAMP (Brazil), and Albert Heijn (Netherlands) to deliver scalable demand
and supply planning solutions.</li>
    <li>Applied statistical techniques such as ARIMA, TBATS, TES, and Random Forest for demand forecasting and lifecycle-
based segmentation.</li>
    <li>Performed data cleansing and anomaly detection to improve forecast accuracy and remove noise from client actuals.</li>
    <li>Analyzed trends and identified gaps between client actuals and forecasts to inform decision-making.</li>
    <li>Disaggregated data from DC/Warehouse to store level for precise analysis and replenishment planning.</li>
    <li>Built Python plugins to automate segmentation and forecasting pipelines aligned to project requirements.</li>
    <li>Participated in design discussions with Project Managers and Solution Architects to evaluate feasibility of deliverables.</li>
    <li>Developed and configured IBPL (Integrated Business Planning Language) procedures and Active Rules for event
triggers and business logic in the o9 Platform.</li>
    <li>Built client-facing dashboards and modules on the o9 Platform for visibility into areas like Replenishment, Capacity
Management, Financial Impact, and Logistics.</li>
    <li>Contributed to Supply Planning Solver configurations to generate feasible supply outputs based on client constraints.</li>
    <li>Prepared testing documentation and sprint reports to guide client users and track implementation progress.</li>
    <li>Utilized tools such as SSIS for batch orchestration and MySQL for data management and transformation.</li>
    <li>Created test scripts and documentation to support clients through onboarding and platform adoption.</li>
</ul>
<h3>Internship (Mar 2022 – Aug 2022):</h3>
<ul>
    <li>Worked with clients including Estée Lauder and ExxonMobil to enhance performance on large datasets and reduce
solver run-times.</li>
    <li>Conducted data profiling, cleaning, and transformation using Python (Pandas) and MySQL.</li>
    <li>Improved batch orchestration logic in SSIS pipelines to optimize execution times for supply chain runs.</li>
    <li>Identified and resolved performance bottlenecks in data processing and scheduling workflows.</li>
</ul>
<h4>Technology Stack:</h4>
<p>Python, MySQL, SQL Optimization (query tuning, indexing strategies, stored procedures), SSIS, Batch
Scheduling & Automation (cron jobs, task schedulers), Tableau (KPI Dashboard Design), MS Office, IBPL (o9 Proprietary
Language), Demand Forecasting (ARIMA, TES, TBATS, RF), Statistical Segmentation, Data Cleansing, Agile (user stories,
sprint planning).</p>
<h2>Technical Skills:</h2>
<h4>Programming Languages:</h4> <p>Python, SQL, R, Bash, SparkSQL</p>
<h4>Cloud Technologies:</h4> <p>AWS (EC2, S3, Lambda, ECS, Secrets Manager, IAM, CloudWatch), Microsoft Azure
(Synapse, Data Factory, Azure Data Lake, Azure Functions)</p>
<h4>Big Data & Distributed Computing:</h4> <p>Apache Spark, Databricks, Delta Lake, Apache Parquet</p>
<h4>Data Engineering & ETL:</h4> <p>Databricks Notebooks, Delta Live Tables, Snowflake Streams & Tasks, Time Travel,
Secure Views, Unity Catalog, Stored Procedures, Batch Scheduling, Error Handling</p>
<h4>Database Servers:</h4> <p>MySQL, PostgreSQL, Snowflake</p>
<h4>Workflow Orchestration:</h4> <p>SSIS, Azure Data Factory, Databricks Workflows</p>
<h4>AI/ML & Analytics:</h4> <p>scikit-learn, Pandas, NumPy, Hugging Face, OpenAI APIs, Feature Engineering, Model
Evaluation</p>
<h4>Visualization & BI Tools:</h4> <p>Tableau, Power BI, Excel</p>
<h4>DevOps & Development Tools:</h4> <p>Jupyter, VS Code, DBFS, Postman</p>
<h4>Web Technologies:</h4> <p>HTML, CSS</p>
<h4>Version Control:</h4> <p>Git, GitHub</p>
<h4>Platforms:</h4> <p>Windows, Linux, Mac</p>
<h2>Awards and certifications</h2>
<h3>Awards</h3>
<h4>Runner-Up – PatriotHacks 2024</h4>
<h5>George Mason University Hackathon</h5>
<ul>
    <li>Built “MasonMetrics”, a scalable data engineering pipeline that ingests, processes, and visualizes student facility usage
data (gyms, libraries, labs) to help optimize campus resource allocation.</li>
    <li>Designed a real-time ETL pipeline using Apache Kafka for data ingestion, Apache Spark for stream processing,
and Amazon Redshift for data warehousing.</li>
    <li>Automated data quality checks and transformation using dbt (data build tool) and integrated Airflow for pipeline
orchestration.</li>
    <li>Developed a dashboard in Tableau showing hourly foot traffic, usage anomalies, and predictive trends using historical
data.</li>
    <li>Earned Runner-Up out of 30+ teams for innovation, impact, and robust architecture</li>
</ul>
<h4>Participant – National Code Camp</h4>
<h5>24-Hour Programming Challenge</h5>
<ul>
    <li>Developed an AI-powered customer support chatbot capable of understanding and responding to user queries in real
time using Natural Language Processing (NLP) with spaCy and transformer models (BERT).</li>
    <li>Designed the backend with Python and deployed it using Flask, integrated with a MongoDB database for storing chat
histories and user feedback.</li>
    <li>Implemented sentiment analysis to escalate negative queries and used Twilio API for optional SMS-based responses.</li>
    <li>Collaborated in a 3-person team, delivering a functional prototype within 24 hours, with focus on scalability, API
integrations, and user experience.</li>
</ul>
<h3>Certifications</h3>
<ul>
    <li>AWS Cloud Foundations</li>
    <li>AWS Associate Solutions Architect</li>
    <li>Databricks Lakehouse Fundamentals</li>
    <li>Snowflake Essentials</li>
    <li>Tableau</li>
    <li>MySQL</li>
</ul>
<hr />
<a href="./hobbies.html">Hobbies</a>
<a href="./contact.html">Contact</a>
</body>
<footer>
    <small>Vijaya Krishna *********</small>
</footer>
</html>